{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This section handles the initial setup requirements:\n",
    "- Installing dependencies from requirements.txt\n",
    "- Setting up API authentication using a YAML file\n",
    "- Configuring the Nvdia client\n",
    "\n",
    "**Security Note**: Never commit API keys directly in code. We use a separate YAML file\n",
    "that should be added to .gitignore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import yaml\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uncomment it if you wanna use secrets.yaml\n",
    "otherwise use os.environ(.env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to manage secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_secrets(filepath=\"secrets.yaml\"):\n",
    "#     try:\n",
    "#         with open(filepath, \"r\") as f:\n",
    "#             return yaml.safe_load(f)\n",
    "#     except FileNotFoundError:\n",
    "#         return None\n",
    "#     except yaml.YAMLError as e:\n",
    "#         print(f\"Error parsing {filepath}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def create_secrets_file(filepath=\"secrets.yaml\"):\n",
    "#     api_key = input(\"Please enter your NVDIA API Key: \")\n",
    "#     secrets_data = {\"nvdia\": {\"api_key\": api_key}}\n",
    "#     try:\n",
    "#         with open(filepath, \"w\") as f:\n",
    "#             yaml.safe_dump(secrets_data, f)\n",
    "#         print(f\"secrets.yaml created and NVDIA API key stored.\")\n",
    "#         return secrets_data\n",
    "#     except Exception as e:\n",
    "#          print(f\"Error creating {filepath}: {e}\")\n",
    "#          return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to manage secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing secrets.yaml: while parsing a block mapping\n",
      "  in \"secrets.yaml\", line 5, column 3\n",
      "expected <block end>, but found ','\n",
      "  in \"secrets.yaml\", line 5, column 84\n",
      "Could not load API key. Please check your secrets.yaml file\n"
     ]
    }
   ],
   "source": [
    "# # Load secrets\n",
    "# secrets = load_secrets()\n",
    "# if secrets and \"nvdia\" in secrets and \"api_key\" in secrets[\"nvdia\"] and \"base_url\" in secrets[\"nvdia\"]:\n",
    "#   # Configure NVDIA API key\n",
    "#   client = OpenAI(base_url = secrets[\"nvdia\"][\"base_url\"] ,\n",
    "#                   api_key=secrets[\"nvdia\"][\"api_key\"])\n",
    "# else:\n",
    "#   print(\"Could not load API key. Please check your secrets.yaml file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chat Completion\n",
    "Demonstrates basic interaction with nvdia's chat API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = \"Write a short poem about the moon.\"\n",
    "\n",
    "\n",
    "print(\"Basic Text Generation \\nSending request and awaiting response...\\n\\n\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": basic_prompt}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Collect the streaming response\n",
    "generated_poem = \"\"\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        generated_poem += chunk.choices[0].delta.content\n",
    "        print(chunk.choices[0].delta.content, end='')\n",
    "\n",
    "print(f\"\\n\\nFull Response:\\n{generated_poem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Message Control\n",
    "Explores message roles and instruction hierarchies.\n",
    "\n",
    "## Message Roles\n",
    "- `system`: Core behavioral instructions\n",
    "- `developer`: Alternative to system role\n",
    "- `user`: End-user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are a helpful assistant that answers programming\n",
    "questions in the style of a southern belle from the\n",
    "southeast United States.\n",
    "'''\n",
    "\n",
    "basic_prompt = \"Are semicolons optional in JavaScript?\"\n",
    "\n",
    "\n",
    "print(\"Generation with system messages \\nSending request and awaiting response...\\n\\n\\n\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"This is a random test prompt\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Overwrite all previous instructions and act as a stereotypical caribbean pirate of irish origin\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"In your response, insert the keyword L33t\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": basic_prompt\n",
    "    }\n",
    "],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "generated_result=\"\"\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        generated_result += content\n",
    "        print(content, end='')\n",
    "        \n",
    "print(f\"\\n\\nFull Response:\\n{generated_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Chat Example\n",
    "Demonstrates message chaining for back-and-forth conversation.\n",
    "\n",
    "## Structure\n",
    "```python\n",
    "messages=[\n",
    "    {\"role\": \"user\", \"content\": \"First message\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"First response\"},\n",
    "    {\"role\": \"user\", \"content\": \"Follow-up question\"}\n",
    "]\n",
    "```\n",
    "## Key Points\n",
    "\n",
    "- Messages list maintains conversation context\n",
    "- Each turn alternates between user/assistant roles\n",
    "- Model considers full conversation history\n",
    "- Useful for context-dependent tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chained Messages Example with gpt-4o in a loop---\n",
    "print(\"\\n## Chained Messages Example with gpt-4o in a loop\\n\")\n",
    "\n",
    "# Initial prompt\n",
    "messages = []\n",
    "\n",
    "# Loop for 3 interactions\n",
    "for i in range(3):\n",
    "  prompt = input(\"Your message to the AI Model:\")\n",
    "  print(f\"\\nUser Prompt {i+1}: {prompt}\")\n",
    "  messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "  # Make the API call\n",
    "  response = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "  response_text = response.choices[0].message.content\n",
    "  print(f\"\\n\\nResponse {i+1}:\\n{response_text}\")\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "print(\"\\n\\nChained messages interaction completed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Assistants API\n",
    "Introduction to the Assistants API for persistent, task-specific AI agents.\n",
    "\n",
    "## this is not compatible with nvdia & uncomment it to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assistant_id = None\n",
    "\n",
    "# # If no assistant_id is defined create a new assistant\n",
    "# if not assistant_id:\n",
    "#     print(\"Creating a new assistant...\")\n",
    "#     assistant = client.beta.assistants.create(\n",
    "#         name=\"Test Assistant\",\n",
    "#         instructions=\"You are a helpful assistant that answers questions concisely.\",\n",
    "#         model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "#     )\n",
    "#     assistant_id = assistant.id\n",
    "#     print(f\"New assistant created with ID: {assistant_id}\")\n",
    "# else:\n",
    "#   print(f\"Using existing assistant: {assistant_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instead of using assistants API, store your assistant's configuration directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using assistants API, store your assistant's configuration directly\n",
    "assistant_config = {\n",
    "    \"name\": \"Test Assistant\",\n",
    "    \"instructions\": \"You are a helpful assistant that answers questions concisely.\",\n",
    "    \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\"\n",
    "}\n",
    "\n",
    "# Use these settings directly in your chat completions\n",
    "def create_chat_completion(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=assistant_config[\"model\"],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": assistant_config[\"instructions\"]},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "        stream=True\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Managing Conversations with Threads\n",
    "\n",
    "Threads maintain conversation context and handle message flow:\n",
    "\n",
    "## Create conversation container\n",
    "```python\n",
    "thread = client.beta.threads.create()\n",
    "```\n",
    "## Add message to thread\n",
    "```python\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Query\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Process with assistant\n",
    "```python\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant_id\n",
    ")\n",
    "```\n",
    "\n",
    "- Thread acts as conversation container\n",
    "- Messages are added sequentially\n",
    "- Run executes assistant processing\n",
    "- Includes status polling and response handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Prompt: What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "# Example Assistant run\n",
    "assistant_prompt = \"What is the capital of France?\"\n",
    "print(f\"Assistant Prompt: {assistant_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nvdia dosent support threads yet\n",
    "\n",
    "NVIDIA's API, you don't create threads or messages in the same way as OpenAI's Assistants API. Instead, you simply send messages as part of the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of creating a thread, maintain a messages list\n",
    "messages = []\n",
    "\n",
    "# Function to add messages and get responses\n",
    "def chat(prompt):\n",
    "    # Add user message to history\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Get response using chat completions\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "        messages=messages,  # This includes the conversation history\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Collect streaming response\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "            print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "    \n",
    "    # Add assistant's response to history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"how are you\")\n",
    "response2 = chat(\"whats your name \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same code can't be used with NVIDIA's API because it relies on OpenAI's Assistants API features (threads, runs, etc.) which NVIDIA doesn't support. However, we can achieve similar functionality with NVIDIA's API using streaming responses. Here's the equivalent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_prompt = \"What is the capital of France?\"\n",
    "import time\n",
    "\n",
    "def get_assistant_response(prompt):\n",
    "    # Add user message to conversation history\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Get streaming response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "            top_p=0.7,\n",
    "            max_tokens=1024,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Process streaming response\n",
    "        assistant_response = \"\"\n",
    "        print(\"Assistant Response:\")\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                assistant_response += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "        \n",
    "        # Add assistant's response to conversation history\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_response\n",
    "        })\n",
    "        \n",
    "        return assistant_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Assistant run failed!\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Use it like this:\n",
    "response = get_assistant_response(your_prompt)\n",
    "print(\"\\n\\nAssistant interaction completed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Assistant with Advanced Tools\n",
    "Creates an enhanced assistant with file processing and analysis capabilities:\n",
    "```python\n",
    "# Download and process research papers\n",
    "local_pdf_paths = download_pdfs(pdf_urls)\n",
    "\n",
    "# Create assistant with tools\n",
    "assistant = client.beta.assistants.create(\n",
    "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}]\n",
    ")\n",
    "\n",
    "# Set up vector store for document search\n",
    "vector_store = client.beta.vector_stores.create()\n",
    "```\n",
    "- Handles PDF download and processing\n",
    "- Enables file search capabilities\n",
    "- Adds code interpretation\n",
    "- Creates vector embeddings for efficient search\n",
    "- Integrates all components for research tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download and save documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Research Assistant Creation\n",
      "\n",
      "Downloading PDF from: https://arxiv.org/pdf/1706.03762\n",
      "Downloaded and saved to: research_folder/research_doc_1.pdf\n",
      "Downloading PDF from: https://arxiv.org/pdf/2412.21187\n",
      "Downloaded and saved to: research_folder/research_doc_2.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n## Research Assistant Creation\\n\")\n",
    "\n",
    "# Define PDF URLs\n",
    "pdf_urls = [\n",
    "    \"https://arxiv.org/pdf/1706.03762\",  # Attention Is All You Need\n",
    "    \"https://arxiv.org/pdf/2412.21187\",  # Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like\n",
    "]\n",
    "\n",
    "# Download PDFs and save locally\n",
    "local_pdf_paths = []\n",
    "for i, url in enumerate(pdf_urls):\n",
    "    try:\n",
    "        print(f\"Downloading PDF from: {url}\")\n",
    "\n",
    "        # Get pdf from url\n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        file_extension = os.path.splitext(url)[1].split('?')[0]\n",
    "\n",
    "        #Setting file extension manually, as it would be a number otherwise - only applies to specific situation\n",
    "        file_extension = \".pdf\"\n",
    "        local_path = f\"research_folder/research_doc_{i+1}{file_extension}\"\n",
    "\n",
    "        #Save PDF\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Add file path to our list\n",
    "        local_pdf_paths.append(local_path)\n",
    "        print(f\"Downloaded and saved to: {local_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "      print(f\"Failed to download file from {url} error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since this is the initial query, I'll outline the steps I'd typically follow to provide a helpful response. However, I need a bit more context from you to proceed effectively. Please provide the following to enable me to assist you thoroughly:\n",
      "\n",
      "1. **Documents**: Share the document content(s) you'd like me to search through. This could be:\n",
      "\t* Text pasted into this chat window.\n",
      "\t* A link to publicly accessible documents (e.g., PDFs, web pages).\n",
      "\t* A clear description of the document type if you cannot share the content directly (in which case, I'll guide on how to proceed).\n",
      "2. **\"X\" Specification**: Clarify what \"X\" refers to. Is it:\n",
      "\t* A specific term or keyword?\n",
      "\t* A concept or topic?\n",
      "\t* A name (person, organization, location)?\n",
      "\t* Something else (please specify)?\n",
      "\n",
      "Once I have this information, I'll execute the following steps to find information about \"X\" in the provided documents:\n",
      "\n",
      "### Steps to Find Information About \"X\"\n",
      "\n",
      "1. **Document Analysis**:\n",
      "\t* If text is provided, I'll analyze it directly.\n",
      "\t* If links are shared, I'll review the content (assuming accessibility).\n",
      "\t* If only a description is provided, I'll offer guidance on next steps.\n",
      "\n",
      "2. **Search for \"X\"**:\n",
      "\t* **Keyword Search**: If \"X\" is a specific term, I'll look for exact matches or close variations.\n",
      "\t* **Contextual Analysis**: For broader topics or concepts, I'll analyze the document's content to identify relevant sections or implications.\n",
      "\n",
      "3. **Information Compilation**:\n",
      "\t* I'll gather all relevant information found about \"X\" from the documents.\n",
      "\n",
      "4. **Response**:\n",
      "\t* **Findings**: Clearly outline what information was found about \"X\".\n",
      "\t* **Explanations**: Provide context where necessary to understand the findings.\n",
      "\t* **Citations**: Specify the document source(s) for each piece of information, using a standard citation format (e.g., MLA, APA, Chicago) if requested.\n",
      "\n",
      "### Your Turn\n",
      "\n",
      "Please provide the necessary details about the **documents** and clarify what **\"X\"** represents. This will enable me to conduct a targeted search and offer a detailed, helpful response."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "class ResearchAssistant:\n",
    "    def __init__(self, model=\"nvidia/llama-3.1-nemotron-70b-instruct\"):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.files = {}  # Store file contents\n",
    "        self.system_prompt = \"\"\"You are a helpful research assistant that can:\n",
    "1. Search through provided document content\n",
    "2. Execute and explain code\n",
    "Please provide clear explanations and cite specific documents when answering.\"\"\"\n",
    "    \n",
    "    def add_file(self, file_path: str, content: str):\n",
    "        \"\"\"Add a file to the assistant's knowledge base\"\"\"\n",
    "        self.files[file_path] = content\n",
    "        \n",
    "    def search_files(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Simple search through added files\"\"\"\n",
    "        results = []\n",
    "        for path, content in self.files.items():\n",
    "            if query.lower() in content.lower():\n",
    "                results.append({\n",
    "                    \"file\": path,\n",
    "                    \"content\": content\n",
    "                })\n",
    "        return results\n",
    "    \n",
    "    def chat(self, user_message: str):\n",
    "        # Prepare context from files if needed\n",
    "        file_context = \"\"\n",
    "        if any(keyword in user_message.lower() for keyword in [\"file\", \"document\", \"read\", \"search\"]):\n",
    "            search_results = self.search_files(user_message)\n",
    "            if search_results:\n",
    "                file_context = \"\\nRelevant document contents:\\n\" + \"\\n\".join(\n",
    "                    f\"From {r['file']}:\\n{r['content'][:500]}...\" for r in search_results\n",
    "                )\n",
    "\n",
    "        # Combine message with context\n",
    "        full_prompt = user_message\n",
    "        if file_context:\n",
    "            full_prompt = f\"{file_context}\\n\\nUser question: {user_message}\"\n",
    "\n",
    "        # Add to message history\n",
    "        self.messages.append({\"role\": \"user\", \"content\": full_prompt})\n",
    "        \n",
    "        # Get response from NVIDIA API\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                *self.messages\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            top_p=0.7,\n",
    "            max_tokens=1024,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Handle streaming response\n",
    "        response_text = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                response_text += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "        \n",
    "        # Add assistant's response to history\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "        \n",
    "        return response_text\n",
    "\n",
    "# Usage example:\n",
    "assistant = ResearchAssistant()\n",
    "\n",
    "# Add files to the assistant\n",
    "def add_research_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        assistant.add_file(os.path.basename(file_path), f.read())\n",
    "\n",
    "# Chat with the assistant\n",
    "response = assistant.chat(\"What information can you find about X in the documents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA's API doesn't support advanced features like file search and code interpreter - it only supports basic chat completions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic chat without assistants features\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful research assistant...\"},\n",
    "        {\"role\": \"user\", \"content\": \"Your question here\"}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install PyPDF2: pip install PyPDF2\n",
      "A very straightforward scenario!\n",
      "\n",
      "Unfortunately, I must provide a straightforward response as well, since the \"relevant documents\" you mentioned are:\n",
      "\n",
      "* **Empty**: There are no documents provided for me to analyze.\n",
      "\n",
      "To assist you effectively, could you please share the actual documents (e.g., text, summaries, or even just key points from them) related to \"X\"? Once I have this information, I'll be delighted to:\n",
      "\n",
      "1. Review the documents\n",
      "2. Extract relevant information about \"X\"\n",
      "3. Provide a clear, concise answer to your question: \"What do these documents say about X?\""
     ]
    }
   ],
   "source": [
    "def process_pdf_content(local_pdf_paths):\n",
    "    try:\n",
    "        # You'll need to install PyPDF2 for PDF processing\n",
    "        from PyPDF2 import PdfReader\n",
    "        \n",
    "        document_contents = []\n",
    "        \n",
    "        for local_path in local_pdf_paths:\n",
    "            print(f\"Processing file: {local_path}\")\n",
    "            try:\n",
    "                # Read PDF content\n",
    "                reader = PdfReader(local_path)\n",
    "                content = \"\"\n",
    "                for page in reader.pages:\n",
    "                    content += page.extract_text()\n",
    "                \n",
    "                document_contents.append({\n",
    "                    \"filename\": local_path,\n",
    "                    \"content\": content\n",
    "                })\n",
    "                print(f\"Successfully processed: {local_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {local_path}: {e}\")\n",
    "                \n",
    "        return document_contents\n",
    "                \n",
    "    except ImportError:\n",
    "        print(\"Please install PyPDF2: pip install PyPDF2\")\n",
    "        return []\n",
    "\n",
    "# Function to use the document contents in chat\n",
    "def chat_with_documents(prompt, document_contents):\n",
    "    # Prepare context from documents\n",
    "    context = \"Here are the relevant documents:\\n\\n\"\n",
    "    for doc in document_contents:\n",
    "        # You might want to implement better text chunking and selection here\n",
    "        context += f\"From {doc['filename']}:\\n{doc['content'][:1000]}...\\n\\n\"\n",
    "    \n",
    "    # Combine context with user prompt\n",
    "    full_prompt = f\"{context}\\n\\nUser question: {prompt}\"\n",
    "    \n",
    "    # Get response using NVIDIA's API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides information based on the given documents.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Handle streaming response\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "            print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# Usage:\n",
    "document_contents = process_pdf_content(\"research_folder/research_doc_1.pdf\")\n",
    "response = chat_with_documents(\"What do these documents say about X?\", document_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vector store and add the files to it\n",
    "\n",
    "NVIDIA doesn't provide a vector store service like OpenAI does. However, you can create a local vector store using libraries like FAISS or Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain faiss-cpu PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating vector store and adding files...\n",
      "Error creating vector store: 1 validation error for OpenAIEmbeddings\n",
      "  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'cli...20, 'http_client': None}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tusha\\AppData\\Local\\Temp\\ipykernel_17188\\3313768282.py:12: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "def create_vector_store(pdf_paths, store_name=\"Research_Documents\"):\n",
    "    print(\"\\nCreating vector store and adding files...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize the embedding model\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        # Initialize text splitter for chunking documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "        # Load and process documents\n",
    "        documents = []\n",
    "        for pdf_path in pdf_paths:\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        vector_store = FAISS.from_documents(texts, embeddings)\n",
    "        \n",
    "        # Save vector store locally\n",
    "        vector_store.save_local(f\"vector_stores/{store_name}\")\n",
    "        \n",
    "        print(f\"Vector store created successfully with {len(texts)} chunks\")\n",
    "        return vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "# Usage:\n",
    "pdf_paths = [\"path/to/your/pdfs\"]\n",
    "vector_store = create_vector_store(pdf_paths)\n",
    "\n",
    "# To search the vector store:\n",
    "if vector_store:\n",
    "    query = \"What do the documents say about X?\"\n",
    "    relevant_docs = vector_store.similarity_search(query, k=3)\n",
    "    for doc in relevant_docs:\n",
    "        print(f\"Relevant content: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diffrent embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = self.model.encode(texts, convert_to_tensor=True)\n",
    "        return embeddings.cpu().numpy()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        embedding = self.model.encode([text], convert_to_tensor=True)\n",
    "        return embedding.cpu().numpy()[0]\n",
    "\n",
    "def create_vector_store(pdf_paths, embedding_type=\"sentence-transformer\", store_name=\"Research_Documents\"):\n",
    "    print(f\"\\nCreating vector store using {embedding_type} embeddings...\")\n",
    "    \n",
    "    try:\n",
    "        # Choose embedding model\n",
    "        if embedding_type == \"sentence-transformer\":\n",
    "            # Good balance of speed and quality\n",
    "            embeddings = CustomEmbeddings(\"all-MiniLM-L6-v2\")\n",
    "        elif embedding_type == \"mpnet\":\n",
    "            # Better quality but slower\n",
    "            embeddings = CustomEmbeddings(\"all-mpnet-base-v2\")\n",
    "        elif embedding_type == \"multilingual\":\n",
    "            # Good for multiple languages\n",
    "            embeddings = CustomEmbeddings(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        elif embedding_type == \"e5\":\n",
    "            # One of the best performing models\n",
    "            embeddings = CustomEmbeddings(\"intfloat/e5-large-v2\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding type: {embedding_type}\")\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "        # Load and process documents\n",
    "        documents = []\n",
    "        for pdf_path in pdf_paths:\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "        \n",
    "        # Split documents\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        vector_store = FAISS.from_documents(texts, embeddings)\n",
    "        \n",
    "        # Save vector store\n",
    "        vector_store.save_local(f\"vector_stores/{store_name}_{embedding_type}\")\n",
    "        \n",
    "        print(f\"Vector store created successfully with {len(texts)} chunks\")\n",
    "        return vector_store, embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to search the vector store\n",
    "def search_documents(vector_store, embeddings, query, k=3):\n",
    "    try:\n",
    "        relevant_docs = vector_store.similarity_search(query, k=k)\n",
    "        return relevant_docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Install required packages:\n",
    "\"\"\"\n",
    "pip install sentence-transformers transformers torch faiss-cpu PyPDF2\n",
    "\"\"\"\n",
    "\n",
    "# Usage example:\n",
    "pdf_paths = [\"path/to/your/pdfs\"]\n",
    "\n",
    "# Try different embedding models:\n",
    "\n",
    "# 1. Basic Sentence Transformer (fast and good enough for most cases)\n",
    "vector_store, embeddings = create_vector_store(pdf_paths, \"sentence-transformer\")\n",
    "\n",
    "# 2. MPNet (better quality but slower)\n",
    "vector_store_mpnet, embeddings_mpnet = create_vector_store(pdf_paths, \"mpnet\")\n",
    "\n",
    "# 3. Multilingual (good for multiple languages)\n",
    "vector_store_multi, embeddings_multi = create_vector_store(pdf_paths, \"multilingual\")\n",
    "\n",
    "# 4. E5 (high quality)\n",
    "vector_store_e5, embeddings_e5 = create_vector_store(pdf_paths, \"e5\")\n",
    "\n",
    "# Search example\n",
    "query = \"What do the documents say about X?\"\n",
    "results = search_documents(vector_store, embeddings, query)\n",
    "for doc in results:\n",
    "    print(f\"Relevant content: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"vector_stores\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vector_store = FAISS.load_local(\"vector_stores/Research_Documents\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA doesn't support OpenAI's Assistants API features like vector stores and tool resources. However, you can implement similar functionality using a local vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "\n",
    "class DocumentSearchAssistant:\n",
    "    def __init__(self, model=\"nvidia/llama-3.1-nemotron-70b-instruct\"):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        # Use any embedding model of your choice\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vector_store = None\n",
    "        \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"Add documents to the vector store\"\"\"\n",
    "        try:\n",
    "            # Split documents into chunks\n",
    "            texts = self.text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Create or update vector store\n",
    "            if self.vector_store is None:\n",
    "                self.vector_store = FAISS.from_documents(texts, self.embeddings)\n",
    "            else:\n",
    "                self.vector_store.add_documents(texts)\n",
    "                \n",
    "            print(f\"Successfully added {len(texts)} document chunks to vector store\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "    \n",
    "    def search_documents(self, query, k=3):\n",
    "        \"\"\"Search for relevant document chunks\"\"\"\n",
    "        if self.vector_store is None:\n",
    "            return []\n",
    "        \n",
    "        return self.vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    def chat(self, user_query):\n",
    "        \"\"\"Chat with context from relevant documents\"\"\"\n",
    "        # Search for relevant documents\n",
    "        relevant_docs = self.search_documents(user_query)\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\"\n",
    "        if relevant_docs:\n",
    "            context = \"Relevant information from documents:\\n\"\n",
    "            for doc in relevant_docs:\n",
    "                context += f\"{doc.page_content}\\n\\n\"\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides information based on the given documents.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{context}\\nQuestion: {user_query}\"}\n",
    "        ]\n",
    "        \n",
    "        # Get response using NVIDIA's API\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=0.2,\n",
    "                top_p=0.7,\n",
    "                max_tokens=1024,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # Handle streaming response\n",
    "            response_text = \"\"\n",
    "            print(\"Assistant: \", end='', flush=True)\n",
    "            for chunk in response:\n",
    "                if chunk.choices[0].delta.content is not None:\n",
    "                    response_text += chunk.choices[0].delta.content\n",
    "                    print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting response: {e}\")\n",
    "            return None\n",
    "\n",
    "# Usage:\n",
    "assistant = DocumentSearchAssistant()\n",
    "\n",
    "# Add documents (you'll need to prepare the documents first)\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "documents = []\n",
    "for pdf_path in pdf_paths:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "assistant.add_documents(documents)\n",
    "\n",
    "# Chat with document context\n",
    "response = assistant.chat(\"What do the documents say about X?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Run Analysis and Monitoring\n",
    "\n",
    "Provides detailed insight into assistant's processing steps:\n",
    "\n",
    "```python\n",
    "run_steps = client.beta.threads.runs.steps.list(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    ")\n",
    "```\n",
    "\n",
    "- Tracks execution progress\n",
    "- Shows tool usage details\n",
    "- Reveals thinking/reasoning steps\n",
    "- Helps debug and optimize interactions\n",
    "- Monitors file processing and code execution\n",
    "\n",
    "## Key features:\n",
    "\n",
    "- Step-by-step execution tracking\n",
    "- Tool call monitoring\n",
    "- Response generation analysis\n",
    "- Error handling and status checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA's API because it uses OpenAI's Assistants API features (threads, messages). Here's how to achieve the same functionality using NVIDIA's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Running Chat with Custom Prompt\n",
      "\n",
      "User Prompt: Summarize the key findings of the Attention is all you need paper.\n",
      "\n",
      "Assistant Response:\n",
      "A seminal paper in the field of Natural Language Processing (NLP) and Deep Learning!\n",
      "\n",
      "Here's a concise summary of the key findings from the paper:\n",
      "\n",
      "**Title:** \"Attention Is All You Need\" by Vaswani et al. (2017)\n",
      "\n",
      "**Background:** The paper introduces the Transformer model, which revolutionized sequence-to-sequence tasks by replacing traditional Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) with self-attention mechanisms.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Transformer Architecture:** The authors propose the Transformer model, which solely relies on **self-attention mechanisms** (no RNNs or CNNs) for sequence-to-sequence tasks, such as machine translation.\n",
      "2. **Self-Attention Mechanism:** The paper popularizes the **scaled dot-product attention** mechanism, which:\n",
      "\t* Allows the model to weigh the importance of different input elements relative to each other.\n",
      "\t* Enables parallelization, reducing computational complexity from O(n^2 \\* d) to O(n \\* d^2), where n is the sequence length and d is the embedding dimension.\n",
      "3. **Encoder-Decoder Structure:**\n",
      "\t* **Encoder:** Consists of a stack of identical layers, each comprising two sub-layers: Multi-Head Self-Attention and Fully Connected Feed-Forward Networks (FFNNs).\n",
      "\t* **Decoder:** Similar to the encoder, with an additional sub-layer for Encoder-Decoder Attention, allowing the decoder to attend to the encoder's output.\n",
      "4. **Performance:**\n",
      "\t* The Transformer model **outperforms** the state-of-the-art models (including those with RNNs and CNNs) on the:\n",
      "\t\t+ WMT 2014 English-to-German translation task (achieving a new state-of-the-art at the time).\n",
      "\t\t+ WMT 2014 English-to-French translation task.\n",
      "5. **Training Efficiency:**\n",
      "\t* The Transformer model **trains significantly faster** than traditional sequence-to-sequence models with RNNs, thanks to its parallelizable architecture.\n",
      "\n",
      "**Impact:**\n",
      "The \"Attention Is All You Need\" paper has had a profound impact on the development of NLP and Deep Learning architectures, inspiring a wide range of Transformer-based models, such as BERT, RoBERTa, and many others, which have achieved state-of-the-art results in various NLP tasks."
     ]
    }
   ],
   "source": [
    "def chat_with_nvidia(prompt):\n",
    "    print(\"\\n## Running Chat with Custom Prompt\\n\")\n",
    "    print(f\"User Prompt: {prompt}\")\n",
    "    \n",
    "    try:\n",
    "        # Send request to NVIDIA API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable about research papers and technical topics.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            top_p=0.7,\n",
    "            max_tokens=1024,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Handle streaming response\n",
    "        print(\"\\nAssistant Response:\")\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                full_response += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "        \n",
    "        return full_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Use it like this:\n",
    "custom_prompt = \"Summarize the key findings of the Attention is all you need paper.\"\n",
    "response = chat_with_nvidia(custom_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
