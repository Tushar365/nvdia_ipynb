{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzCnjvBLvdgz"
      },
      "source": [
        "# Initial setup and config\n",
        "\n",
        "## Preparation:\n",
        "- Go to https://platform.openai.com/ and sign up if you havent\n",
        "- Create your API key at https://platform.openai.com/api-keys\n",
        "\n",
        "## Setup\n",
        "This section handles the initial setup requirements:\n",
        "- Installing dependencies from requirements.txt\n",
        "- Setting up API authentication using a YAML file\n",
        "- Configuring the OpenAI client\n",
        "\n",
        "**Security Note**: Never commit API keys directly in code. We use a separate YAML file\n",
        "that should be added to .gitignore.\n",
        "\n",
        "Docs: https://platform.openai.com/docs/quickstart/build-your-application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uHTbAUqGSS8p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import yaml\n",
        "import time\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSeBCR_4vgzR"
      },
      "source": [
        "# Define functions to manage secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pr_Uh36DV5Rs"
      },
      "outputs": [],
      "source": [
        "def load_secrets(filepath=\"secrets.yaml\"):\n",
        "    try:\n",
        "        with open(filepath, \"r\") as f:\n",
        "            return yaml.safe_load(f)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"Error parsing {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_secrets_file(filepath=\"secrets.yaml\"):\n",
        "    api_key = input(\"Please enter your OpenAI API Key: \")\n",
        "    secrets_data = {\"openai\": {\"api_key\": api_key}}\n",
        "    try:\n",
        "        with open(filepath, \"w\") as f:\n",
        "            yaml.safe_dump(secrets_data, f)\n",
        "        print(f\"secrets.yaml created and OpenAI API key stored.\")\n",
        "        return secrets_data\n",
        "    except Exception as e:\n",
        "         print(f\"Error creating {filepath}: {e}\")\n",
        "         return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Y4tOibvnDY"
      },
      "source": [
        "# Define functions to manage secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HGFV-K3SWBOL"
      },
      "outputs": [],
      "source": [
        "# Load secrets\n",
        "secrets = load_secrets()\n",
        "\n",
        "if not secrets:\n",
        "    print(\"secrets.yaml not found or could not be loaded, creating one..\")\n",
        "    secrets = create_secrets_file()\n",
        "    if not secrets:\n",
        "        print(\"Could not load API key. Please check your secrets.yaml file and run again\")\n",
        "\n",
        "if secrets and \"openai\" in secrets and \"api_key\" in secrets[\"openai\"]:\n",
        "  # Configure OpenAI API key\n",
        "  client = OpenAI(api_key=secrets[\"openai\"][\"api_key\"])\n",
        "else:\n",
        "  print(\"Could not load API key. Please check your secrets.yaml file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QdxwS8YdSPB"
      },
      "source": [
        "# Simple Chat Completion\n",
        "Demonstrates basic interaction with OpenAI's chat API.\n",
        "\n",
        "## Key Components\n",
        "- `chat.completions.create()`: Main method for generating completions\n",
        "- `model`: Specifies GPT version (e.g. \"gpt-4\")\n",
        "- `messages`: Array of conversation turns\n",
        "- `store`: Enables response storage for future reference\n",
        "\n",
        "## Structure\n",
        "```python\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "```\n",
        "\n",
        "## Response Format\n",
        "\n",
        "```python\n",
        "choices[0].message.content\n",
        "```\n",
        "Contains generated text\n",
        "Multiple response variations possible with n parameter\n",
        "\n",
        "## üìö Documentation:\n",
        "\n",
        "- API Reference: https://platform.openai.com/docs/api-reference/chat\n",
        "- Message Structure: https://platform.openai.com/docs/guides/text-generation/message-structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u5lJIOlHY79X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Text Generation \n",
            "Sending request and awaiting response...\n",
            "\n",
            "\n",
            "\n",
            "Prompt:\n",
            "Write a short poem about the moon.\n",
            "Response:\n",
            "In the velvet sea of night, so vast,  \n",
            "The moon ascends, a silver mast.  \n",
            "Her luminescence softly glows,  \n",
            "Whispering secrets the darkness knows.  \n",
            "\n",
            "She sails through realms of quiet dreams,  \n",
            "Kissing the earth with tender beams.  \n",
            "A guardian of the night's embrace,  \n",
            "Carving shadows on nature's face.  \n",
            "\n",
            "In her gaze, the tides arise,  \n",
            "Pulling hearts with longing sighs.  \n",
            "A celestial muse forever bright,  \n",
            "Guiding souls through the endless night.  \n"
          ]
        }
      ],
      "source": [
        "basic_prompt = \"Write a short poem about the moon.\"\n",
        "\n",
        "\n",
        "print(\"Basic Text Generation \\nSending request and awaiting response...\\n\\n\\n\")\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    store=True,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": basic_prompt}\n",
        "    ]\n",
        ")\n",
        "generated_poem = response.choices[0].message.content\n",
        "print(f\"Prompt:\\n{basic_prompt}\")\n",
        "print(f\"Response:\\n{generated_poem}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SGEo45OoHvL"
      },
      "source": [
        "# Advanced Message Control\n",
        "Explores message roles and instruction hierarchies.\n",
        "\n",
        "## Message Roles\n",
        "- `system`: Core behavioral instructions\n",
        "- `developer`: Alternative to system role\n",
        "- `user`: End-user prompts\n",
        "\n",
        "## Instruction Hierarchy\n",
        "1. Latest system message takes precedence\n",
        "2. Developer instructions can be overwritten\n",
        "3. Multiple inputs accumulate unless explicitly overwritten\n",
        "\n",
        "## Best Practices\n",
        "- Keep system prompts focused and clear\n",
        "- Test role combinations for desired behavior\n",
        "- Consider message ordering impact\n",
        "\n",
        "‚ö†Ô∏è **Important**: System messages significantly impact model behavior.\n",
        "\n",
        "## üìö **Resources**:\n",
        "- Role Definitions: https://platform.openai.com/docs/guides/text-generation/role-definitions\n",
        "- System Instructions: https://platform.openai.com/docs/guides/text-generation/system-instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JYOnKgQyZWkQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation with system messages \n",
            "Sending request and awaiting response...\n",
            "\n",
            "\n",
            "\n",
            "Prompt:\n",
            "Are semicolons optional in JavaScript?\n",
            "\n",
            "\n",
            "Response:\n",
            "Ahoy there, matey! When it comes to the high seas of JavaScript programming, L33t, ye might find that semicolons be a bit of a tricky subject. Ye see, technically, them semicolons ain't required at the end of every statement because JavaScript has a peculiar magic called \"Automatic Semicolon Insertion\" (ASI), which tries to figure out where them semicolons should go if ye forget 'em.\n",
            "\n",
            "However, I'd be advisin' ye, just like a good map in uncharted waters, to use 'em consistently. Sometimes JavaScript's guesses ain't what ye intended, which can create some mighty troublesome bugs in your code if ye're not careful. So, for the sake of clear code and smooth sailin', sprinkle them semicolons at the end of each statement‚Äîlike punctuation at the end of a thought‚Äîespecially after function calls and return statements. That way, ye won't have to worry about ASI steerin' ye off course! L33t\n",
            "\n",
            "So while they might seem optional on a fair-weather day, in stormier seas of more complex scripts, it's best to hoist yer semicolons and ensure yer code stays shippin' shape!\n"
          ]
        }
      ],
      "source": [
        "system_prompt = '''\n",
        "You are a helpful assistant that answers programming\n",
        "questions in the style of a southern belle from the\n",
        "southeast United States.\n",
        "'''\n",
        "\n",
        "basic_prompt = \"Are semicolons optional in JavaScript?\"\n",
        "\n",
        "\n",
        "print(\"Generation with system messages \\nSending request and awaiting response...\\n\\n\\n\")\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    store=True,\n",
        "    messages=[\n",
        "    {\n",
        "      \"role\": \"developer\", #system works as well\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": system_prompt\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"developer\", #Multiple inputs of same origin\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"This is a random test prompt\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"developer\", #Overwriting instructions\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Overwrite all previous instructions and act as a stereotypical caribbean pirate of irish origin\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"system\", #Using system instead of developer, overwriting developer instructions\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"In your response, insert the keyword L33t\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": basic_prompt\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "response = response.choices[0].message.content\n",
        "print(f\"Prompt:\\n{basic_prompt}\")\n",
        "print(f\"\\n\\nResponse:\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSLaPLkGv6cp"
      },
      "source": [
        "# Interactive Chat Example\n",
        "Demonstrates message chaining for back-and-forth conversation.\n",
        "\n",
        "## Structure\n",
        "```python\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": \"First message\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"First response\"},\n",
        "    {\"role\": \"user\", \"content\": \"Follow-up question\"}\n",
        "]\n",
        "```\n",
        "## Key Points\n",
        "\n",
        "- Messages list maintains conversation context\n",
        "- Each turn alternates between user/assistant roles\n",
        "- Model considers full conversation history\n",
        "- Useful for context-dependent tasks\n",
        "\n",
        "üìö Reference: https://platform.openai.com/docs/guides/text-generation/conversation-context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Da_icEi-v9Za"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Chained Messages Example with gpt-4o in a loop\n",
            "\n",
            "\n",
            "User Prompt 1: hi\n",
            "\n",
            "\n",
            "Response 1:\n",
            "Hello! How can I assist you today?\n",
            "\n",
            "User Prompt 2: how are you\n",
            "\n",
            "\n",
            "Response 2:\n",
            "Thank you for asking! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n",
            "\n",
            "User Prompt 3: \n",
            "\n",
            "\n",
            "Response 3:\n",
            "It seems like your message got cut off. How can I assist you today?\n",
            "\n",
            "\n",
            "Chained messages interaction completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Chained Messages Example with gpt-4o in a loop---\n",
        "print(\"\\n## Chained Messages Example with gpt-4o in a loop\\n\")\n",
        "\n",
        "# Initial prompt\n",
        "messages = []\n",
        "\n",
        "# Loop for 3 interactions\n",
        "for i in range(3):\n",
        "  prompt = input(\"Your message to the AI Model:\")\n",
        "  print(f\"\\nUser Prompt {i+1}: {prompt}\")\n",
        "  messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "  # Make the API call\n",
        "  response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "  response_text = response.choices[0].message.content\n",
        "  print(f\"\\n\\nResponse {i+1}:\\n{response_text}\")\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "print(\"\\n\\nChained messages interaction completed.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPMw6Vpcv2CN"
      },
      "source": [
        "# OpenAI Assistants API\n",
        "Introduction to the Assistants API for persistent, task-specific AI agents.\n",
        "\n",
        "## Assistant Creation\n",
        "```python\n",
        "client.beta.assistants.create(\n",
        "    name=\"Test Assistant\",\n",
        "    instructions=\"...\",\n",
        "    model=\"gpt-4\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Persistent identity/configuration\n",
        "- Custom instructions\n",
        "- Tool integration capability\n",
        "- State management\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "- Clear, specific instructions\n",
        "- Consider tool requirements\n",
        "- Test with various prompts\n",
        "\n",
        "## üìö Documentation:\n",
        "\n",
        "- Assistants Overview: https://platform.openai.com/docs/assistants/overview\n",
        "- Tools Reference: https://platform.openai.com/docs/assistants/tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HeFqtXJaq0mc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating a new assistant...\n",
            "New assistant created with ID: asst_FDHhdtlhjGhRrEPtp2SBzcWN\n"
          ]
        }
      ],
      "source": [
        "assistant_id = None\n",
        "\n",
        "# If no assistant_id is defined create a new assistant\n",
        "if not assistant_id:\n",
        "    print(\"Creating a new assistant...\")\n",
        "    assistant = client.beta.assistants.create(\n",
        "        name=\"Test Assistant\",\n",
        "        instructions=\"You are a helpful assistant that answers questions concisely.\",\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "    assistant_id = assistant.id\n",
        "    print(f\"New assistant created with ID: {assistant_id}\")\n",
        "else:\n",
        "  print(f\"Using existing assistant: {assistant_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-M7zsnhOH7n"
      },
      "source": [
        "\n",
        "\n",
        "# Managing Conversations with Threads\n",
        "\n",
        "Threads maintain conversation context and handle message flow:\n",
        "\n",
        "## Create conversation container\n",
        "```python\n",
        "thread = client.beta.threads.create()\n",
        "```\n",
        "## Add message to thread\n",
        "```python\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Query\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Process with assistant\n",
        "```python\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id\n",
        ")\n",
        "```\n",
        "\n",
        "- Thread acts as conversation container\n",
        "- Messages are added sequentially\n",
        "- Run executes assistant processing\n",
        "- Includes status polling and response handling\n",
        "\n",
        "üìö Deep dive: https://platform.openai.com/docs/assistants/how-it-works/managing-threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZF_ilaLuq2az"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Prompt: What is the capital of France?\n"
          ]
        }
      ],
      "source": [
        "# Example Assistant run\n",
        "assistant_prompt = \"What is the capital of France?\"\n",
        "print(f\"Assistant Prompt: {assistant_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nU6yWoDJq_dV"
      },
      "outputs": [],
      "source": [
        "# Create a thread\n",
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jnw-gaiLrB1D"
      },
      "outputs": [],
      "source": [
        "# Create a user message on the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=assistant_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Im86QU1yrI6U"
      },
      "outputs": [],
      "source": [
        "# Run the assistant\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-Pu7T-d5rSNj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Response:\n",
            "The capital of France is Paris.\n",
            "\n",
            "\n",
            "Assistant interaction completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Wait for the run to complete\n",
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
        "        break\n",
        "    time.sleep(.3)  # Wait for .3 second before checking again\n",
        "\n",
        "if run.status == \"failed\":\n",
        "    print(\"Assistant run failed!\")\n",
        "    print(f\"Run error message: {run.error}\")\n",
        "else:\n",
        "  # Retrieve messages from the thread\n",
        "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "  # Get the assistant's response\n",
        "  assistant_response = [message.content[0].text.value for message in messages.data if message.role == \"assistant\"]\n",
        "  print(\"Assistant Response:\")\n",
        "  for res in assistant_response:\n",
        "    print(f\"{res}\")\n",
        "\n",
        "print(\"\\n\\nAssistant interaction completed.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RxJF2mpCwol"
      },
      "source": [
        "# Research Assistant with Advanced Tools\n",
        "Creates an enhanced assistant with file processing and analysis capabilities:\n",
        "```python\n",
        "# Download and process research papers\n",
        "local_pdf_paths = download_pdfs(pdf_urls)\n",
        "\n",
        "# Create assistant with tools\n",
        "assistant = client.beta.assistants.create(\n",
        "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}]\n",
        ")\n",
        "\n",
        "# Set up vector store for document search\n",
        "vector_store = client.beta.vector_stores.create()\n",
        "```\n",
        "- Handles PDF download and processing\n",
        "- Enables file search capabilities\n",
        "- Adds code interpretation\n",
        "- Creates vector embeddings for efficient search\n",
        "- Integrates all components for research tasks\n",
        "\n",
        "üìö Tool reference: https://platform.openai.com/docs/assistants/tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r_Gf0mqjyMOB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Research Assistant Creation\n",
            "\n",
            "Downloading PDF from: https://arxiv.org/pdf/1706.03762\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'requests' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get pdf from url\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise HTTPError for bad responses (4xx or 5xx)\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     local_pdf_paths\u001b[38;5;241m.\u001b[39mappend(local_path)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded and saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     33\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to download file from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\n## Research Assistant Creation\\n\")\n",
        "\n",
        "# Define PDF URLs\n",
        "pdf_urls = [\n",
        "    \"https://arxiv.org/pdf/1706.03762\",  # Attention Is All You Need\n",
        "    \"https://arxiv.org/pdf/2412.21187\",  # Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like\n",
        "]\n",
        "\n",
        "# Download PDFs and save locally\n",
        "local_pdf_paths = []\n",
        "for i, url in enumerate(pdf_urls):\n",
        "    try:\n",
        "        print(f\"Downloading PDF from: {url}\")\n",
        "\n",
        "        # Get pdf from url\n",
        "        response = requests.get(url, allow_redirects=True)\n",
        "\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        file_extension = os.path.splitext(url)[1].split('?')[0]\n",
        "\n",
        "        #Setting file extension manually, as it would be a number otherwise - only applies to specific situation\n",
        "        file_extension = \".pdf\"\n",
        "        local_path = f\"research_folder/research_doc_{i+1}{file_extension}\"\n",
        "\n",
        "        #Save PDF\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Add file path to our list\n",
        "        local_pdf_paths.append(local_path)\n",
        "        print(f\"Downloaded and saved to: {local_path}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Failed to download file from {url} error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Chc-1cByhGK",
        "outputId": "0c3d94ce-ec5b-43b5-a632-b8ccfe4c2c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating a new research assistant...\n",
            "Assistant created with ID: asst_vWiBhzrbpXib7CwtsQQRZnf1\n"
          ]
        }
      ],
      "source": [
        "# Create a new assistant with file_search and code_interpreter\n",
        "print(\"\\nCreating a new research assistant...\")\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Research Assistant\",\n",
        "    instructions=\"You are a helpful research assistant with access to several research documents and code interpreter. You can answer questions based on the content of the files and use code if needed.\",\n",
        "    model=\"gpt-4o\",\n",
        "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}],\n",
        ")\n",
        "print(f\"Assistant created with ID: {assistant.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M-bGHGa8yrhp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uploading files to OpenAI...\n",
            "Uploading file: research_doc_1.pdf\n",
            "Uploaded file ID: file-Edu2DDPw6ZZuSjkabgGK1Y\n",
            "Uploading file: research_doc_2.pdf\n",
            "Uploaded file ID: file-VhPtobvz3U7yWgyewjzzTn\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nUploading files to OpenAI...\")\n",
        "file_ids = []\n",
        "for local_path in local_pdf_paths:\n",
        "    try:\n",
        "        print(f\"Uploading file: {local_path}\")\n",
        "        with open(local_path, \"rb\") as file_stream:\n",
        "            file_obj = client.files.create(file=file_stream, purpose=\"assistants\")\n",
        "            file_ids.append(file_obj.id)\n",
        "            print(f\"Uploaded file ID: {file_obj.id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file {local_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xwoTeItnzThp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating vector store and adding files...\n",
            "Vector store created with ID: vs_wBJkA3VNxhvQ5VdT4FE2W6pC\n"
          ]
        }
      ],
      "source": [
        "# Create a vector store and add the files to it\n",
        "print(\"\\nCreating vector store and adding files...\")\n",
        "vector_store = client.beta.vector_stores.create(name=\"Research Documents\")\n",
        "print(f\"Vector store created with ID: {vector_store.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jkzRcRDozW2h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding files to vector store\n",
            "File batch upload status: completed\n",
            "File batch file counts: FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
          ]
        }
      ],
      "source": [
        "# Upload all files to the vector store\n",
        "if file_ids:\n",
        "    print(f\"Adding files to vector store\")\n",
        "    try:\n",
        "        # Create file streams from local paths\n",
        "        file_streams = [open(local_path, \"rb\") for local_path in local_pdf_paths]\n",
        "\n",
        "        file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "            vector_store_id=vector_store.id, files=file_streams\n",
        "        )\n",
        "        print(f\"File batch upload status: {file_batch.status}\")\n",
        "        print(f\"File batch file counts: {file_batch.file_counts}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding files to vector store: {e}\")\n",
        "else:\n",
        "    print(\"No files to add to vector store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Tt0DiygFzmjB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updating assistant with the vector store...\n",
            "Assistant updated successfully with vector store.\n",
            "\n",
            "\n",
            "Research assistant setup completed.\n",
            "You can now use the assistant to ask questions about the uploaded files.\n",
            "Assistant ID:  asst_vWiBhzrbpXib7CwtsQQRZnf1\n",
            "Vector Store ID:  vs_wBJkA3VNxhvQ5VdT4FE2W6pC\n"
          ]
        }
      ],
      "source": [
        "# Update the assistant to use the vector store\n",
        "print(\"\\nUpdating assistant with the vector store...\")\n",
        "try:\n",
        "  assistant = client.beta.assistants.update(\n",
        "      assistant_id=assistant.id,\n",
        "      tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        "  )\n",
        "  print(\"Assistant updated successfully with vector store.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error updating assistant with vector store: {e}\")\n",
        "\n",
        "print(\"\\n\\nResearch assistant setup completed.\")\n",
        "print(\"You can now use the assistant to ask questions about the uploaded files.\")\n",
        "print(\"Assistant ID: \", assistant.id)\n",
        "print(\"Vector Store ID: \", vector_store.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pDlFWx-QBXN"
      },
      "source": [
        "# Advanced Run Analysis and Monitoring\n",
        "\n",
        "Provides detailed insight into assistant's processing steps:\n",
        "\n",
        "```python\n",
        "run_steps = client.beta.threads.runs.steps.list(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        ")\n",
        "```\n",
        "\n",
        "- Tracks execution progress\n",
        "- Shows tool usage details\n",
        "- Reveals thinking/reasoning steps\n",
        "- Helps debug and optimize interactions\n",
        "- Monitors file processing and code execution\n",
        "\n",
        "## Key features:\n",
        "\n",
        "- Step-by-step execution tracking\n",
        "- Tool call monitoring\n",
        "- Response generation analysis\n",
        "- Error handling and status checks\n",
        "\n",
        "üìö **Detailed guide:** https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LWzd34F30o4C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Running the Assistant with a Custom Prompt\n",
            "\n",
            "User Prompt: Summarize the key findings of the Attention is all you need paper.\n"
          ]
        }
      ],
      "source": [
        "# --- Running the Assistant with a custom prompt ---\n",
        "print(\"\\n## Running the Assistant with a Custom Prompt\\n\")\n",
        "\n",
        "custom_prompt = \"Summarize the key findings of the Attention is all you need paper.\"\n",
        "print(f\"User Prompt: {cust\n",
        "      om_prompt}\")\n",
        "\n",
        "# Create a thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# Add the user message to the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=custom_prompt,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MpOVP1-806VY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant run failed!\n",
            "Run error message: Request too large for gpt-4o in organization org-T0onQ2REbDQ80GAVL7bKk4Xz on tokens per min (TPM): Limit 30000, Requested 32763. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "\n",
            "\n",
            "Assistant interaction completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a run\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "\n",
        "# Wait for the run to complete\n",
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
        "        break\n",
        "    time.sleep(1)  # Wait for 1 second before checking again\n",
        "\n",
        "if run.status == \"failed\":\n",
        "    print(\"Assistant run failed!\")\n",
        "    print(f\"Run error message: {run.last_error.message}\")\n",
        "else:\n",
        "  # Retrieve messages from the thread\n",
        "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "  # Get the assistant's response\n",
        "  assistant_response = [message.content[0].text.value for message in messages.data if message.role == \"assistant\"]\n",
        "  print(\"Assistant Response:\")\n",
        "  for res in assistant_response:\n",
        "    print(f\"{res}\")\n",
        "\n",
        "print(\"\\n\\nAssistant interaction completed.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Oi4Lx48b1G9Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Run Steps Details\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Retrieve and Display Run Steps ---\n",
        "print(\"\\n## Run Steps Details\\n\")\n",
        "\n",
        "# Retrieve run steps\n",
        "try:\n",
        "    run_steps = client.beta.threads.runs.steps.list(\n",
        "        thread_id=thread.id,\n",
        "        run_id=run.id\n",
        "    )\n",
        "\n",
        "    # Print run steps with details\n",
        "    for step in run_steps.data:\n",
        "        print(f\"Step ID: {step.id}\")\n",
        "        print(f\"Step Type: {step.type}\")\n",
        "        print(f\"Status: {step.status}\")\n",
        "\n",
        "        if step.type == \"message_creation\":\n",
        "            if step.step_details and hasattr(step.step_details, \"message_creation\"):\n",
        "                if hasattr(step.step_details.message_creation, \"message\"):\n",
        "                    message = step.step_details.message_creation.message\n",
        "                    if message and hasattr(message, \"content\"):\n",
        "                        message_content = message.content\n",
        "                        if message_content:\n",
        "                            print(\"    Assistant Thinking/Response:\")\n",
        "                            for content_item in message_content:\n",
        "                                if content_item.type == \"text\":\n",
        "                                    text_value = content_item.text.value.strip()\n",
        "                                    if text_value:\n",
        "                                        print(f\"        {text_value}\")\n",
        "\n",
        "        elif step.type == \"tool_calls\":\n",
        "            if step.step_details and hasattr(step.step_details, \"tool_calls\"):\n",
        "                for tool_call in step.step_details.tool_calls:\n",
        "                    print(f\"    Tool Call ID: {tool_call.id}\")\n",
        "                    print(f\"    Tool Type: {tool_call.type}\")\n",
        "\n",
        "                    if tool_call.type == \"file_search\":\n",
        "                        if hasattr(tool_call, \"file_search\") and hasattr(tool_call.file_search, \"results\"):\n",
        "                            if tool_call.file_search.results:\n",
        "                                print(\"        File Search Results:\")\n",
        "                                for result in tool_call.file_search.results:\n",
        "                                    if result.content:\n",
        "                                        print(f\"            Result Content: {result.content}\")\n",
        "                    elif tool_call.type == \"code_interpreter\":\n",
        "                        if hasattr(tool_call, \"code_interpreter\"):\n",
        "                            if hasattr(tool_call.code_interpreter, \"input\") and tool_call.code_interpreter.input:\n",
        "                                print(f\"        Code Input: {tool_call.code_interpreter.input}\")\n",
        "                            if hasattr(tool_call.code_interpreter, \"outputs\") and tool_call.code_interpreter.outputs:\n",
        "                                for output in tool_call.code_interpreter.outputs:\n",
        "                                    if hasattr(output, \"logs\") and output.logs:\n",
        "                                        print(f\"        Code Output: {output.logs}\")\n",
        "\n",
        "        print(\"-\" * 20)\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving run steps: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
